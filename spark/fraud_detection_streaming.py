import os
import sys
import json
import time

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType
from spark.spark_config import get_spark_session

KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
TOPIC = os.getenv('KAFKA_TOPIC', 'transactions')
POSTGRES_URL = os.getenv('POSTGRES_URL', 'jdbc:postgresql://localhost:5432/fintech_db')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'fintech_user')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'fintech_pass')
CHECKPOINT = os.getenv('SPARK_CHECKPOINT_DIR', '/tmp/spark-checkpoints')

schema = StructType() 
schema = schema.add('transaction_id', StringType())
schema = schema.add('user_id', StringType())
schema = schema.add('timestamp', StringType())
schema = schema.add('merchant_category', StringType())
schema = schema.add('amount', DoubleType())
schema = schema.add('location', StringType())


def write_to_postgres(df, table_name: str, mode: str = 'append'):
    # For transactions and fraud_alerts, we need to handle duplicates
    # Use a custom function to insert with ON CONFLICT
    
    if df.isEmpty():
        return
    
    # Collect the data (only works for small batches in streaming)
    rows = df.collect()
    
    if not rows:
        return
    
    # Get column names
    columns = df.columns
    
    # Build INSERT ... ON CONFLICT query
    if table_name == 'transactions':
        pk_column = 'transaction_id'
    elif table_name == 'fraud_alerts':
        pk_column = 'alert_id'
    else:
        pk_column = 'id'
    
    # Create connection and insert
    import psycopg2
    
    conn = psycopg2.connect(
        host='localhost',
        port=5432,
        database='fintech_db',
        user=POSTGRES_USER,
        password=POSTGRES_PASSWORD
    )
    
    cursor = conn.cursor()
    
    # Build INSERT query with ON CONFLICT
    placeholders = ','.join(['%s'] * len(columns))
    columns_str = ','.join(columns)
    
    # For fraud_alerts, we don't have a primary key yet, so use regular insert
    if table_name == 'fraud_alerts':
        query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
    else:
        query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders}) ON CONFLICT ({pk_column}) DO NOTHING"
    
    # Insert each row
    for row in rows:
        try:
            cursor.execute(query, tuple(row))
        except Exception as e:
            print(f"Error inserting row: {e}")
            # Continue with other rows
    
    conn.commit()
    cursor.close()
    conn.close()


def main():
    spark = get_spark_session('fraud-detection-stream')

    raw = spark.readStream.format('kafka') \
        .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP) \
        .option('subscribe', TOPIC) \
        .option('startingOffsets', 'latest') \
        .option('failOnDataLoss', 'false') \
        .load()

    json_df = raw.selectExpr("CAST(value AS STRING) as json_str") \
        .select(F.from_json('json_str', schema).alias('data')) \
        .select('data.*') \
        .withColumn('event_time', F.to_timestamp('timestamp')) \
        .withColumn('processing_time', F.current_timestamp())

    # High value detection
    high_value = json_df.filter(F.col('amount') > 5000) \
        .withColumn('fraud_type', F.lit('HIGH_VALUE')) \
        .withColumn('detection_time', F.current_timestamp()) \
        .withColumn('details', F.to_json(F.struct('*')))

    # Impossible travel detection: windowed check for different locations per user
    # We'll mark events where within a 10-minute window the same user has >1 distinct location
    windowed = json_df.withWatermark('event_time', '2 minutes') \
        .groupBy(
            F.window('event_time', '10 minutes', '5 minutes'),
            'user_id'
        ).agg(F.collect_set('location').alias('locations'), F.max('processing_time').alias('last_processing')) \
        .filter(F.size('locations') > 1) \
        .select(F.col('user_id'), F.col('window'), F.col('locations'), F.col('last_processing')) \
        .withColumn('fraud_type', F.lit('IMPOSSIBLE_TRAVEL')) \
        .withColumn('detection_time', F.current_timestamp())

    # For impossible travel we need to write detail rows for each transaction involved. For simplification
    # we'll create alert rows with details containing the window and locations.
    impossible_alerts = windowed.select(
        'user_id',
        F.concat(F.lit('user-'), F.col('user_id')).alias('transaction_id'),
        F.col('fraud_type'),
        F.col('detection_time'),
        F.to_json(F.struct('window', 'locations')).alias('details')
    )

    # Prepare high_value alerts to match fraud_alerts schema (alert_id will be auto-generated by PostgreSQL)
    high_value_alerts = high_value.select(
        'user_id',
        'transaction_id',
        F.col('fraud_type'),
        F.col('detection_time'),
        'details'
    )

    # Write all incoming transactions to `transactions` table
    transactions_out = json_df.select(
        'transaction_id', 'user_id', 'event_time', 'merchant_category', 'amount', 'location', 'processing_time'
    )

    # Streams: write transactions and write alerts
    transactions_query = transactions_out.writeStream \
        .foreachBatch(lambda df, epoch_id: write_to_postgres(df, 'transactions')) \
        .outputMode('append') \
        .option('checkpointLocation', CHECKPOINT + '/transactions') \
        .start()

    high_value_query = high_value_alerts.writeStream \
        .foreachBatch(lambda df, epoch_id: write_to_postgres(df, 'fraud_alerts')) \
        .outputMode('append') \
        .option('checkpointLocation', CHECKPOINT + '/high_value_alerts') \
        .start()

    impossible_query = impossible_alerts.writeStream \
        .foreachBatch(lambda df, epoch_id: write_to_postgres(df, 'fraud_alerts')) \
        .outputMode('append') \
        .option('checkpointLocation', CHECKPOINT + '/impossible_alerts') \
        .start()

    spark.streams.awaitAnyTermination()


if __name__ == '__main__':
    main()
